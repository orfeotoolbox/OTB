\chapter{Classification}

\section{Introduction}

Image classification consists in extracting added-value information from images. 
Such processing methods classify pixels within images into geographical connected 
zones with similar properties, and identified by a common class label. The 
classification can be either unsupervised or supervised.

Unsupervised classification does not require any additional information about the
properties of the input image to classify it. On the contrary, supervised methods 
need a preliminary learning to be computed over training datasets having similar 
properties than the image to classify, in order to build a classification model.

%In statistical classification, each object is represented by $d$ features (a
%measurement vector), and the goal of classification becomes finding compact and
%disjoint regions (decision regions\cite{Duda2000}) for classes in a
%$d$-dimensional feature space. Such decision regions are defined by decision
%rules that are known or can be trained.  The simplest configuration of a
%classification consists of a decision rule and multiple membership functions;
%each membership function represents a class. Figure~\ref{fig:simple}
%illustrates this general framework.

%\begin{figure}[h]
%  \centering
%  \includegraphics[width=0.7\textwidth]{DudaClassifier.eps}
%  \itkcaption[Simple conceptual classifier]{Simple conceptual classifier.}
%  \label{fig:simple}
%\end{figure}

%This framework closely follows that of Duda and
%Hart\cite{Duda2000}. The classification process can be described
%as follows:

%\begin{enumerate}
%\item{A measurement vector is input to each membership function.}
%\item{Membership functions feed the membership scores to the
%    decision rule.}
%\item{A decision rule compares the membership scores and returns a
%    class label.}
%\end{enumerate}

%\begin{figure}
%  \centering
%  \includegraphics[width=0.7\textwidth]{StatisticalClassificationFramework.eps}
%  \itkcaption[Statistical classification framework]{Statistical classification
%framework.}
%  \protect\label{fig:StatisticalClassificationFramework}
%\end{figure}

%This simple configuration can be used to formulated various classification
%tasks by using different membership functions and incorporating task specific
%requirements and prior knowledge into the decision rule. For example, instead
%of using probability density functions as membership functions, through
%distance functions and a minimum value decision rule (which assigns a class
%from the distance function that returns the smallest value) users can achieve a
%least squared error classifier. As another example, users can add a rejection
%scheme to the decision rule so that even in a situation where the membership
%scores suggest a ``winner'', a measurement vector can be flagged as ill
%defined. Such a rejection scheme can avoid risks of assigning a class label
%without a proper win margin.

%Note that to use these concepts into your own programs, you might have to link
%to other OTB libraries (edit the \code{TARGET\_LINK\_LIBRARIES} to add them),
%for example OTBLearning, OTBMarkov.

\section{Unsupervised classification}

\subsection{K-Means Classification}
\label{sec:KMeansClassifier}

\subsubsection{Simple version}
\ifitkFullVersion
\input{ScalarImageKmeansClassifier.tex}
\fi
\ifitkFullVersion
\input{ScalarImageKmeansModelEstimator.tex}
\fi

\subsubsection{General approach}
\ifitkFullVersion
\input{KMeansImageClassificationExample.tex}
\fi

\subsubsection{k-d Tree Based k-Means Clustering}
\label{sec:KdTreeBasedKMeansClustering}
\ifitkFullVersion
\input{KdTreeBasedKMeansClustering.tex}
\fi

\subsection{Kohonen's Self Organizing Map}
\label{sec:SOM}
\input{Kohonen}
%%%1. Construction SOM
\subsubsection{Building a color table}
\label{sec:SOMColorTable}
\input{SOMExample}
\subsubsection{SOM Classification}
\label{sec:SOMClassification}
\input{SOMClassifierExample}

\subsubsection{Multi-band, streamed classification}

\ifitkFullVersion
\input{SOMImageClassificationExample.tex}
\fi

%%%2. Lecture SOM et ensemble de vecteurs autre image pour construire
%%%ActivationMAP 

%\subsection{Bayesian classification}

\subsection{Bayesian Plug-In Classifier}
\label{sec:BayesianPluginClassifier}

\ifitkFullVersion 
\input{BayesianPluginClassifier.tex}
\fi


\subsection{Expectation Maximization Mixture Model Estimation}
\label{sec:ExpectationMaximizationMixtureModelEstimation}

\ifitkFullVersion 
\input{ExpectationMaximizationMixtureModelEstimator.tex}
\fi




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Statistical Segmentations}
\label{sec:StatisticalSegmentations}

%\subsection{Markov Random Fields}

\subsubsection{Stochastic Expectation Maximization}
\label{sec:SEM}

The Stochastic Expectation Maximization (SEM) approach is a stochastic 
version of the EM mixture estimation seen on
section~\ref{sec:ExpectationMaximizationMixtureModelEstimation}. It has been 
introduced by \cite{CeDi95} to prevent convergence of the EM approach from
local minima. It avoids the analytical maximization issued by integrating a
stochastic sampling procedure in the estimation process. It induces an almost
sure (a.s.) convergence to the algorithm.

From the initial two step formulation of the EM mixture estimation, the SEM
may be decomposed into 3 steps:
\begin{enumerate}
\item \textbf{E-step}, calculates the expected membership values for each 
measurement vector to each classes.
\item \textbf{S-step}, performs a stochastic sampling of the membership vector
to each classes, according to the membership values computed in the E-step.
\item \textbf{M-step}, updates the parameters of the membership probabilities
(parameters to be defined through the class
\subdoxygen{itk}{Statistics}{ModelComponentBase} and its inherited classes).
\end{enumerate}
The implementation of the SEM has been turned to a contextual SEM in the sense
where the evaluation of the membership parameters is conditioned to
membership values of the spatial neighborhood of each pixels.

\ifitkFullVersion 
\input{SEMModelEstimatorExample.tex}
\fi


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Classification using Markov Random Fields}
\label{sec:MarkovRandomField}

Markov Random Fields are probabilistic models that use the statistical
dependency between
pixels in a neighborhood to infeer the value of a give pixel.

\subsubsection{ITK framework}
\label{sec:MarkovRandomFieldITK}
The
\subdoxygen{itk}{Statistics}{MRFImageFilter} uses the maximum a posteriori (MAP)
estimates for modeling the MRF. The object traverses the data set and uses the
model generated by the Mahalanobis distance classifier to get the the distance
between each pixel in the data set to a set of known classes, updates the
distances by evaluating the influence of its neighboring pixels (based on a MRF
model) and finally, classifies each pixel to the class which has the minimum
distance to that pixel (taking the neighborhood influence under consideration).
The energy function minimization is done using the iterated conditional modes
(ICM) algorithm \cite{Besag1986}.

\ifitkFullVersion
\input{ScalarImageMarkovRandomField1.tex}
\fi 

\subsubsection{OTB framework}
\label{sec:MarkovRandomFieldOTB}
The ITK approach was considered not to be flexible enough for some
remote sensing applications. Therefore, we decided to implement our
own framework.
\index{Markov}

\begin{figure}[th]
  \centering
  \includegraphics[width=0.7\textwidth]{MarkovFramework.eps}
  \itkcaption[OTB Markov Framework]{OTB Markov Framework.}
  \label{fig:markovFramework}
\end{figure}

\index{Markov!Classification}
\ifitkFullVersion
\input{MarkovClassification1Example.tex}
\fi 

\index{Markov!Classification}
\ifitkFullVersion
\input{MarkovClassification2Example.tex}
\fi 

\index{Markov!Classification}
\ifitkFullVersion
\input{MarkovClassification3Example.tex}
\fi 

\index{Markov!Regularization}
\ifitkFullVersion
\input{MarkovRegularizationExample.tex}
\fi 




\section{Supervised classification}

\subsection{Generic machine learning framework}
\label{sec:MLGenericFramework}

The OTB supervised classification is implemented as a generic Machine Learning 
framework, supporting several possible machine learning library as backends.
As of now both libSVM (the machine learning library historically integrated in OTB)
and the machin learning methods of OpenCV library (\cite{opencv_library}) are available.

The current list of classifiers available through the same generic interface within the OTB is:

\begin{itemize}
  \item \textbf{LibSVM}: Support Vector Machines classifier based on libSVM.
  \item \textbf{SVM}: Support Vector Machines classifier based on OpenCV, itself based on libSVM.
  \item \textbf{Bayes}: Normal Bayes classifier based on OpenCV.
  \item \textbf{Boost}: Boost classifier based on OpenCV.
  \item \textbf{DT}: Decision Tree classifier based on OpenCV.
  \item \textbf{RF}: Random Forests classifier based on the Random Trees in OpenCV.
  \item \textbf{GBT}: Gradient Boosted Tree classifier based on OpenCV.
  \item \textbf{KNN}: K-Nearest Neighbors classifier based on OpenCV.
  \item \textbf{ANN}: Artificial Neural Network classifier based on OpenCV.
\end{itemize}


\subsection{An example of supervised classification method: Support Vector Machines}
\label{sec:SupportVectorMachines}

\subsubsection{SVM general description}
Kernel based learning methods in general and the Support Vector
Machines (SVM) in particular, have been introduced in the last years
in learning theory for classification and regression tasks,
\cite{vapnik}. SVM have been successfully applied to text
categorization, \cite{joachims}, and face recognition,
\cite{osuna}. Recently, they have been successfully used for the
classification of hyperspectral remote-sensing images, \cite{bruzzoneSVM}.

Simply stated, the approach consists in searching for the separating
surface between 2 classes by the determination of the subset of
training samples which best describes the boundary between the 2
classes. These samples are called support vectors and completely
define the classification system. In the case where the two classes are
nonlinearly separable, the method uses a kernel expansion in order to make
projections of the feature space onto higher dimensionality spaces
where the separation of the classes becomes linear.


\subsubsection{SVM mathematical formulation}

This subsection reminds the basic principles of SVM learning and
classification. A good tutorial on SVM can be found in, \cite{burges}.
 
We have $N$ samples represented by the couple $(y_i,\mathbf{x}_i),
i=1\ldots N$ where $y_i \in \{-1,+1\}$ is the class label and
$\mathbf{x}_i \in \mathbb{R}^n$ is the feature vector of dimension
$n$. A classifier is a function  $$f(\mathbf{x},\boldsymbol{\alpha}) :
\mathbf{x}\mapsto y$$ where $\boldsymbol{\alpha}$ are the classifier
parameters. The SVM finds the optimal separating hyperplane which
fulfills the following constraints :
    \begin{itemize}
      \item The samples with labels $+1$ and $-1$ are on different
      sides of the hyperplane.
      \item The distance of the closest vectors to the hyperplane is
      maximised. These are the support vectors (SV) and this distance is
      called the margin.
    \end{itemize}

    The separating hyperplane has the equation
    $$\mathbf{w}\cdot\mathbf{x}+b=0;$$ with $\mathbf{w}$ being its
    normal vector and $x$ being any point of the hyperplane. The
    orthogonal distance to the origin is given by
    $\frac{|b|}{\|\mathbf{w}\|}$. Vectors located outside the
    hyperplane have either $\mathbf{w}\cdot\mathbf{x}+b>0$ or
      $\mathbf{w}\cdot\mathbf{x}+b<0$.

    Therefore, the classifier function can be written as
    $$f(\mathbf{x},\mathbf{w}, b)=sgn(\mathbf{w}\cdot\mathbf{x}+b).$$
    
The SVs are placed on two hyperplanes which are parallel to the
      optimal separating one. In order to find the optimal
      hyperplane, one sets $\mathbf{w}$ and
      $b$ : $$\mathbf{w}\cdot\mathbf{x}+b=\pm 1.$$

Since there must not be any vector inside the margin, the following
constraint can be used:
    $$\mathbf{w}\cdot\mathbf{x}_i+b\ge +1\text{ if }y_i=+1;$$
    $$\mathbf{w}\cdot\mathbf{x}_i+b\le -1\text{ if }y_i=-1;$$ which
    can be rewritten as $$y_i(\mathbf{w}\cdot\mathbf{x}_i+b)-1\ge 0~  ~ \forall i.$$

    The orthogonal distances of the 2 parallel hyperplanes to the
    origin are $\frac{|1-b|}{\|\mathbf{w}\|}$ and
      $\frac{|-1-b|}{\|\mathbf{w}\|}$. Therefore the modulus of the
    margin is equal to $\frac{2}{\|\mathbf{w}\|}$ and it has to be
    maximised.

    Thus, the problem to be solved is:

	\begin{itemize}
	\item Find $\mathbf{w}$ and $b$ which minimise
	 $\left\{ \frac{1}{2}\|\mathbf{w}\|^2 \right\}$
	\item under the constraint :
	 $y_i(\mathbf{w}\cdot\mathbf{x}_i+b)\ge 1~  ~ i=1\ldots N.$
	\end{itemize}

	This problem can be solved by using the Lagrange multipliers
	with one multiplier per sample. It can be shown that only the
	support vectors will have a positive Lagrange multiplier.

	In the case where the two classes are not exactly linearly
	separable, one can modify the constraints above by using 
      $$\mathbf{w}\cdot\mathbf{x}_i+b\ge +1 - \xi_i \text{ if }y_i=+1;$$
    $$\mathbf{w}\cdot\mathbf{x}_i+b\le -1+\xi_i \text{ if }y_i=-1;$$
    $$\xi_i\ge 0~  ~\forall i.$$

	If $\xi_i > 1$, one considers that the sample is wrong. The
	function which has then to be minimised is
	$\frac{1}{2}\|\mathbf{w}\|^2 + C\left( \sum_i \xi_i\right); $,
	where $C$ is a tolerance parameter. The optimisation problem
	is the same than in the linear case, but one multiplier has to
	be added for each new constraint $\xi_i\ge 0$.

	If the decision surface needs to be non-linear, this solution
	cannot be applied and the kernel approach has to be adopted.


One drawback of the SVM is that, in their basic version, they can only
solve two-class problems. Some works exist in the field of multi-class
SVM (see \cite{allwein00reducing,weston98multiclass}, and the
comparison made by \cite{hsu01comparison}), but they are
not used in our system.

You have to be aware that to achieve better convergence of the algorithm it is 
strongly advised to normalize feature vector components in the $[-1;1]$ 
interval.

For problems with $N > 2$ classes, one can choose either to train $N$
SVM (one class against all the others), or to train $N\times(N-1)$ SVM
(one class against each of the others). In the second approach, which
is the one that we use, the final decision is taken by choosing the
class which is most often selected by the whole set of SVM.



\subsection{Learning from samples}
\label{ssec:LearningFromSamples}
\input{TrainMachineLearningModelFromSamplesExample.tex}

\subsection{Learning from images}
\label{ssec:LearningFromImages}
\input{TrainMachineLearningModelFromImagesExample.tex}


%\subsection{Learning With PointSets}
%\label{sec:LearningWithPointSets}
%\input{SVMPointSetModelEstimatorExample}

%\subsection{PointSet Classification}
%\label{sec:PointSetClassification}
%\input{SVMPointSetClassificationExample}

%\subsection{Learning With Images}
%\label{sec:LearningWithImages}
%\input{SVMImageModelEstimatorExample}

%\subsection{Image Classification}
%\label{sec:ImageClassification}
%\input{SVMImageClassificationExample}
%\input{SVMImageEstimatorClassificationMultiExample}


\subsection{Multi-band, streamed classification}

\ifitkFullVersion
\input{SupervisedImageClassificationExample.tex}
\fi




\subsection{Generic Kernel SVM}
OTB has developed a specific interface for user-defined kernels. However, the 
following functions use a deprecated OTB interface. A function 
$k(\cdot,\cdot)$ is considered to be a kernel when:
\begin{align}\label{eqMercer}
        \forall g(\cdot) \in {\cal L}^2(\mathbbm{R}^n) \quad & \text{so 
that} \quad
        \int g(\boldsymbol{x})^2 d\boldsymbol{x} \text{ be finite,} \\
        & \text{then} \quad \int k(\boldsymbol{x},\boldsymbol{y}) \, 
g(\boldsymbol{x})
        \, g(\boldsymbol{y}) \, d\boldsymbol{x} d\boldsymbol{y} \geqslant 0,
        \notag
\end{align}
which is known as the {\em Mercer condition\/}.

When defined through the OTB, a kernel is a class that inherits from
\code{GenericKernelFunctorBase}. Several virtual functions have to 
be overloaded:
\begin{itemize}
\item The \code{Evaluate} function, which implements the behavior of the 
kernel
itself. For instance, the classical linear kernel could be re-implemented
with:
\begin{verbatim}
        double
        MyOwnNewKernel
        ::Evaluate ( const svm_node * x, const svm_node * y,
                     const svm_parameter & param ) const
        {
                return this->dot(x,y);
        }
\end{verbatim}
This simple example shows that the classical dot product is already 
implemented
into \subdoxygen{otb}{GenericKernelFunctorBase}{dot()} as a protected
function.
\item The \code{Update()} function which synchronizes local variables and 
their
integration into the initial SVM procedure. The following examples will show
the way to use it.
\end{itemize}

Some pre-defined generic kernels have already been implemented in OTB:
\begin{itemize}
\item \doxygen{otb}{MixturePolyRBFKernelFunctor} which implements a 
linear mixture
of a polynomial and a RBF kernel;
\item \doxygen{otb}{NonGaussianRBFKernelFunctor} which implements a non
gaussian RBF kernel;
\item \doxygen{otb}{SpectralAngleKernelFunctor}, a kernel that integrates
the Spectral Angle, instead of the Euclidean distance, into an inverse 
multiquadric kernel.
This kernel may be appropriated when using multispectral data.
\item \doxygen{otb}{ChangeProfileKernelFunctor}, a kernel which is
dedicated to the supervized classification of the multiscale change profile
presented in section \ref{sec:KullbackLeiblerProfile}.
\end{itemize}

\subsubsection{Learning with User Defined Kernels}
\label{sec:Learningwithuserdefinedkernel}
\ifitkFullVersion
\input{SVMGenericKernelImageModelEstimatorExample.tex}
\fi

\subsubsection{Classification with user defined kernel}

\ifitkFullVersion
\input{SVMGenericKernelImageClassificationExample.tex}
\fi




\section{Fusion of Classification maps}

\subsection{General approach of image fusion}
In order to obtain a relevant image classification it is sometimes necessary to 
fuse several classification maps coming from different classification methods 
(SVM, KNN, Random Forest, Artificial Neural Networks,...). The fusion of 
classification maps combines them in a more robust and precise one. Two methods are 
available in the OTB: the majority voting and the Demspter Shafer framework.


\subsection{Majority voting}
\subsubsection{General description}
For each input pixel, the Majority Voting method consists in choosing the more 
frequent class label among all classification maps to fuse. In case of not unique 
more frequent class labels, the undecided value is set for such pixels in 
the fused output image.

\subsubsection{An example of majority voting fusion}
\ifitkFullVersion
\input{MajorityVotingFusionOfClassificationMapsExample.tex}
\fi


\subsection{Dempster Shafer}

\subsubsection{General description}
A more adaptive fusion method using the 
\href{http://en.wikipedia.org/wiki/Dempster\%E2\%80\%93Shafer_theory}{Dempster-Shafer theory} 
is available within the OTB. This method is adaptive as it is based on the 
so-called belief function of each class label for each classification map. Thus, 
each classified pixel is associated to a degree of confidence according to the 
classifier used. In the Dempster Shafer framework, the expert's point of view 
(i.e. with a high belief function) is considered as the truth. In order to 
estimate the belief function of each class label, we use the Dempster Shafer 
combination of masses of belief for each class label and for each classification 
map. In this framework, the output fused label of each pixel is the one with the 
maximal belief function.

Like for the majority voting method, the Dempster Shafer fusion handles not 
unique class labels with the maximal belief function. In this case, the output 
fused pixels are set to the undecided value.

The confidence levels of all the class labels are estimated from a comparision of 
the classification maps to fuse with a ground truth, which results in a 
confusion matrix. For each classification maps, these confusion matrices are then 
used to estimate the mass of belief of each class label.


\subsubsection{Mathematical formulation of the combination algorithm}

A description of the mathematical formulation of the Dempster Shafer combination 
algorithm is available in the following OTB Wiki page:
\href{http://wiki.orfeo-toolbox.org/index.php/Information_fusion_framework}{http://wiki.orfeo-toolbox.org/index.php/} page.

\subsubsection{An example of Dempster Shafer fusion}
\ifitkFullVersion
\input{DempsterShaferFusionOfClassificationMapsExample.tex}
\fi



\section{Classification map regularization}

%\subsection{Regularization by neighborhood-based majority voting}

\ifitkFullVersion
\input{ClassificationMapRegularizationExample.tex}
\fi



